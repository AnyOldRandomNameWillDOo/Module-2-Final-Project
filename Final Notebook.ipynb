{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 2 Final Project Submission\n",
    "* Name: Vivienne DiFrancesco\n",
    "* Pace: Full Time\n",
    "* Instructor: James Irving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T00:11:07.240784Z",
     "start_time": "2020-09-04T00:11:07.159786Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing libraries that I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mtick\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting default seaborn setting for my visuals\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Supressing warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importing the statsmodels packages I will use\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Importing scikit learn packages I will use\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.175216Z",
     "start_time": "2020-09-03T21:26:41.171257Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting pandas to display max columns and rows\n",
    "pd.set_option('display.max_columns', 0)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Turning off scientific notation in pandas\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.290214Z",
     "start_time": "2020-09-03T21:26:41.176216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading in the data\n",
    "df = pd.read_csv('kc_house_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.338219Z",
     "start_time": "2020-09-03T21:26:41.334215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the index to the id\n",
    "df.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.346290Z",
     "start_time": "2020-09-03T21:26:41.340220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking out the length and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.365217Z",
     "start_time": "2020-09-03T21:26:41.347215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the data types and where there might be nulls\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrubbing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addressing the price column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started with the price column since that is the target. I wanted to get to know the data a little using describe(). I looked at value_counts() to make sure there weren't issues with rogue values like 0000 or something that would not register as nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.374217Z",
     "start_time": "2020-09-03T21:26:41.368221Z"
    }
   },
   "outputs": [],
   "source": [
    "# Making price an integer instead of a float\n",
    "df.price = df.price.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.388216Z",
     "start_time": "2020-09-03T21:26:41.377217Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the stats for the column to see if everything looks normal\n",
    "df.price.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.399218Z",
     "start_time": "2020-09-03T21:26:41.389216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Double checking that there aren't rogue values hiding in the data\n",
    "df.price.value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with NA values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then turned to the other columns to deal with NA values. I filled the NA values, cast them to the correct data type, and then used value_counts() to check for rogue entries that may have been missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.413217Z",
     "start_time": "2020-09-03T21:26:41.401216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Looking at all NA values in all columns\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I tried mapping the entries that were missing waterfront and it seems as if some of the entries would be waterfront properties. I decided to fill the null values based on the ratio of 0 and 1 that are already in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.422290Z",
     "start_time": "2020-09-03T21:26:41.415214Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a sub-dataframe of the missing entries to use for visualizing\n",
    "waterfront_check = df.copy()\n",
    "waterfront_check = waterfront_check[waterfront_check['waterfront'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.425217Z",
     "start_time": "2020-09-03T21:26:41.423215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the file\n",
    "\n",
    "# waterfront_check.to_csv(r'C:\\Users\\drudi\\DataScience\\Module02\\FinalProject\\waterfront_check.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This map was created using the waterfront_check dataframe loaded into Tableau Public. This screenshot is a zoomed in view to better see individual entries as an example. The full image can be viewed and downloaded from https://public.tableau.com/profile/vivienne4370 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T03:56:44.344887Z",
     "start_time": "2020-09-02T03:56:44.339888Z"
    }
   },
   "source": [
    "<img src=\"waterfrontcheck.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.440234Z",
     "start_time": "2020-09-03T21:26:41.426249Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the percentages of the different values\n",
    "df.waterfront.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.449218Z",
     "start_time": "2020-09-03T21:26:41.441215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking value counts before filling the missing values\n",
    "df.waterfront.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.557224Z",
     "start_time": "2020-09-03T21:26:41.459215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting the probability ratios based on the value counts\n",
    "prob = df.waterfront.value_counts(normalize=True).values\n",
    "\n",
    "# Setting a seed so that the random results are the same every time\n",
    "np.random.seed(123)\n",
    "\n",
    "# Filling the missing values with either 0 or 1 using the probability\n",
    "df[\"waterfront\"] = df[\"waterfront\"].apply(lambda x: \n",
    "                                          np.random.choice([0, 1], p=prob) \n",
    "                                          if (np.isnan(x)) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.566215Z",
     "start_time": "2020-09-03T21:26:41.558215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Making sure the value counts changed appropriately \n",
    "# and there are no nulls left\n",
    "df[\"waterfront\"].value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.573237Z",
     "start_time": "2020-09-03T21:26:41.567216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Changing the datatype\n",
    "df.waterfront = df.waterfront.astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I dropped the view column since it is not clear what this data represents. It does not represent the views from the house but likely has something to do with listing views. Without knowing what it could mean, I dropped it to avoid any confusion from the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.584224Z",
     "start_time": "2020-09-03T21:26:41.574218Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filling NA values with 0\n",
    "df.drop(columns='view', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I decided to fill the yr_renovated columns with zeros and just assume that if the value is null that it means the house has not been renovated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.591216Z",
     "start_time": "2020-09-03T21:26:41.585215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filling NA values with 0\n",
    "df.yr_renovated.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.599218Z",
     "start_time": "2020-09-03T21:26:41.592216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for rogue values\n",
    "df.yr_renovated.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.608216Z",
     "start_time": "2020-09-03T21:26:41.603225Z"
    }
   },
   "outputs": [],
   "source": [
    "# Changing the datatype\n",
    "df.yr_renovated = df.yr_renovated.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.622214Z",
     "start_time": "2020-09-03T21:26:41.611214Z"
    }
   },
   "outputs": [],
   "source": [
    "# Verifying that all NAs were dealt with\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for strange values in other columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I looked through the rest of my columns for rogue entries and to generally better get to know my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.634215Z",
     "start_time": "2020-09-03T21:26:41.623214Z"
    }
   },
   "outputs": [],
   "source": [
    "# Looking at columns and data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.646214Z",
     "start_time": "2020-09-03T21:26:41.636215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for any strange values\n",
    "df.date.value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There is an entry with 33 bedrooms. I'm going to leave it for now and deal with it later when I do outlier removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.655225Z",
     "start_time": "2020-09-03T21:26:41.647216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for any strange values\n",
    "df.bedrooms.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.663239Z",
     "start_time": "2020-09-03T21:26:41.656217Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for any strange values\n",
    "df.bathrooms.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.676215Z",
     "start_time": "2020-09-03T21:26:41.666256Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for any strange values\n",
    "df.sqft_living.value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.685215Z",
     "start_time": "2020-09-03T21:26:41.677214Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for any strange values\n",
    "df.sqft_lot.value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.696216Z",
     "start_time": "2020-09-03T21:26:41.687220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for any strange values\n",
    "df.floors.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.706215Z",
     "start_time": "2020-09-03T21:26:41.698215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for any strange values\n",
    "df.condition.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.715216Z",
     "start_time": "2020-09-03T21:26:41.708215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for any strange values\n",
    "df.grade.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.726222Z",
     "start_time": "2020-09-03T21:26:41.717216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for any strange values\n",
    "df.sqft_above.value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are question mark values for sqft_basement. I decided to fill them with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.737216Z",
     "start_time": "2020-09-03T21:26:41.727214Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for any strange values\n",
    "df.sqft_basement.value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.759215Z",
     "start_time": "2020-09-03T21:26:41.738215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replacing the ? entries with 0\n",
    "df.sqft_basement = df.sqft_basement.map(lambda x: \n",
    "                                        int(float(x.replace('?', '0'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.764215Z",
     "start_time": "2020-09-03T21:26:41.760215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting the datatype to be an integer\n",
    "df.sqft_basement = df.sqft_basement.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.773215Z",
     "start_time": "2020-09-03T21:26:41.765215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for any strange values again\n",
    "df.sqft_basement.value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.782215Z",
     "start_time": "2020-09-03T21:26:41.775215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for any strange values\n",
    "df.yr_built.value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.791216Z",
     "start_time": "2020-09-03T21:26:41.783216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for any strange values\n",
    "df.zipcode.value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.837216Z",
     "start_time": "2020-09-03T21:26:41.793215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for any strange values\n",
    "df.lat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.849216Z",
     "start_time": "2020-09-03T21:26:41.838216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for any strange values\n",
    "df.long.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.859216Z",
     "start_time": "2020-09-03T21:26:41.851216Z"
    }
   },
   "outputs": [],
   "source": [
    "df.sqft_living15.value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.869215Z",
     "start_time": "2020-09-03T21:26:41.860215Z"
    }
   },
   "outputs": [],
   "source": [
    "df.sqft_lot15.value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.882229Z",
     "start_time": "2020-09-03T21:26:41.870216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Final check that all the data types are good\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some columns I want to add based on the data I have in other columns that I think may be of more use to my model than the current columns. For example the sqft_basement column has mostly 0 entries. It may be more useful to have a column that indicates whether a basement exists or not. Also with dates, it may be useful to have columns based on month or season sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.899215Z",
     "start_time": "2020-09-03T21:26:41.883217Z"
    }
   },
   "outputs": [],
   "source": [
    "# Spliting the month out of the date column into a new column of its own\n",
    "df['month_sold'] = df['date'].map(lambda x: x.replace('/', ' ').split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.908214Z",
     "start_time": "2020-09-03T21:26:41.901215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Casting the new column to be an integer\n",
    "df['month_sold'] = df['month_sold'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.917215Z",
     "start_time": "2020-09-03T21:26:41.910215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the values of the new column\n",
    "df.month_sold.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.923216Z",
     "start_time": "2020-09-03T21:26:41.918215Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Function that takes the day from the date column \n",
    "# and assigns a week of the month\n",
    "\n",
    "def week_of_month(x):\n",
    "#     Splitting the day out from the date\n",
    "    day = int(x.replace('/', ' ').split()[1])\n",
    "    \n",
    "#     Assigning the week based on the day\n",
    "    if day < 8:\n",
    "        week = 1\n",
    "    elif day < 15 and day > 7:\n",
    "        week = 2\n",
    "    elif day < 22 and day > 14:\n",
    "        week = 3\n",
    "    elif day < 29 and day > 21:\n",
    "        week = 4\n",
    "    else:\n",
    "        week = 5\n",
    "        \n",
    "    return week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.953236Z",
     "start_time": "2020-09-03T21:26:41.925215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adding the new column by applying the function above\n",
    "df['week_sold'] = df['date'].map(week_of_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.961248Z",
     "start_time": "2020-09-03T21:26:41.954217Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the values of the new column\n",
    "df.week_sold.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.969256Z",
     "start_time": "2020-09-03T21:26:41.963216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function that takes the month sold and assigns the month to a season\n",
    "\n",
    "def season_sold(x):\n",
    "    \n",
    "#     Assigning season based on month\n",
    "    if x > 2 and x < 6:\n",
    "        season = 1\n",
    "    elif x > 5 and x < 9:\n",
    "        season = 2\n",
    "    elif x > 8 and x < 12:\n",
    "        season = 3\n",
    "    else:\n",
    "        season = 4\n",
    "        \n",
    "    return season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.984248Z",
     "start_time": "2020-09-03T21:26:41.971217Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the new column by applying the function above\n",
    "df['season_sold'] = df['month_sold'].map(season_sold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:41.992216Z",
     "start_time": "2020-09-03T21:26:41.985251Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the values of the new column\n",
    "df.season_sold.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:42.003217Z",
     "start_time": "2020-09-03T21:26:41.994217Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating new renovated column\n",
    "df['was_renovated'] = df['yr_renovated'].map(lambda x: x!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:42.010214Z",
     "start_time": "2020-09-03T21:26:42.004270Z"
    }
   },
   "outputs": [],
   "source": [
    "# Casting the new column to be an integer\n",
    "df['was_renovated'] = df['was_renovated'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:42.018216Z",
     "start_time": "2020-09-03T21:26:42.011216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the values of the new column\n",
    "df.was_renovated.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:42.031216Z",
     "start_time": "2020-09-03T21:26:42.020249Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating new basement column\n",
    "df['has_basement'] = df['sqft_basement'].map(lambda x: x!=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:42.037216Z",
     "start_time": "2020-09-03T21:26:42.033215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Casting the new column to be an integer\n",
    "df['has_basement'] = df['has_basement'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:42.045215Z",
     "start_time": "2020-09-03T21:26:42.039215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the values of the new column\n",
    "df.has_basement.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:42.058292Z",
     "start_time": "2020-09-03T21:26:42.047216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropped the date column since the values will be hard to model with\n",
    "df.drop(columns='date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:42.069294Z",
     "start_time": "2020-09-03T21:26:42.059273Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking all my columns at data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Question 1: How does location affect house prices, sizes, and other metrics? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was curious to do some exploration with the latitude and longitude to map the houses and look at various factors like price, home square footage, and lot square footage. The visuals here were created using my cleaned data loaded into Tableau Public. The full images can be viewed and downloaded from https://public.tableau.com/profile/vivienne4370"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:26:42.074230Z",
     "start_time": "2020-09-03T21:26:42.071215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the file to use\n",
    "# df.to_csv(r'C:\\Users\\drudi\\DataScience\\Module02\\FinalProject\\cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location and price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Home Prices By Location.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Median Home Price By Zipcode.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> These maps show that the areas around Seattle and Bellevue are the most expensive. I am surprised that Bellevue is actually more expensive than Seattle. In general though the trend is that the closer to the urban center, then the more expensive. Which is a trend I expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location and squarefootage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Squarefoot Lot By Location.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Squarefoot Living Space By Location.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As expected the lot size increases the further out you get from the urban center. However, I am surprised to see that in the Bellevue area there are homes with some larger lots. Home squarefootage also surprises me that there are some very large homes in the urban center. I would expect the homes to have more of a trend of being larger the further you get out from the city. That is somewhat the case, but not as dramatically as I would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location and home age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Year Built By Location.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This visualization shows the clear trend of the oldest homes being in the urban center and how houses were built out from there over time. It's interesting to see the smattering of newer homes in the urban center and pockets of older homes on the outskirts. It would be interesting to know more of the history of these old homes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the distribution of prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T01:51:01.941703Z",
     "start_time": "2020-09-04T01:51:01.279601Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting the figure and plotting\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "sns.distplot(df['price'], bins='auto')\n",
    "\n",
    "# Adjusting the money ticks \n",
    "fmt_money = '${x:,.0f}'\n",
    "tick_money = mtick.StrMethodFormatter(fmt_money)\n",
    "ax.xaxis.set_major_formatter(tick_money)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Setting the title and labels\n",
    "ax.set_xlabel('Price', fontsize=15)\n",
    "ax.set_title('House Prices', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">There is a large tail to the distribution of prices which is to be expected since there are of course a few houses that are much more expensive than the majority of houses. I will address all the outliers at a later point, but it's good to see that there looks to still be good normality in the prices despite the long tail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking linearity, normality distribution, and choosing categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T21:59:18.321567Z",
     "start_time": "2020-09-03T21:59:18.311551Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a function to plot a single feature against price\n",
    "def explore_plot(df, col, target='price'):\n",
    "    \n",
    "#     Plotting the graph and setting the regression line to be red\n",
    "    g = sns.jointplot(data=df, x=col, y=target, kind='reg', height=6, \n",
    "                      joint_kws={'line_kws':{'color':'red'}})\n",
    "\n",
    "#     Setting the title\n",
    "    plt.suptitle(f'{col} and {target}', fontsize=20, y=1.05)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T22:00:00.796458Z",
     "start_time": "2020-09-03T21:59:20.405785Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Looping through the columns to feed each into the function above\n",
    "cols = list(df.columns)\n",
    "for col in cols:\n",
    "    explore_plot(df, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Based on these graphs, I decided to make condition, zipcode, month_sold, week_sold, and season_sold categorical features. I want to turn yr_built into a category as well since there is no linearity in the relationship. Others have a very weak linear relationship, but I will leave them for now with my first model and judge them by the p-values later. There are many features that also do not have normality or have a lot of outliers which I will address for other models. For this first model I want to leave the data as is to better judge improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T23:29:34.229535Z",
     "start_time": "2020-09-03T23:29:34.221620Z"
    }
   },
   "outputs": [],
   "source": [
    "categories = ['condition', 'zipcode','month_sold', 'week_sold', 'season_sold']\n",
    "for col in categories:\n",
    "    df[col] = df[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making year built categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T22:42:59.371592Z",
     "start_time": "2020-09-03T22:42:59.358555Z"
    }
   },
   "outputs": [],
   "source": [
    "df.yr_built.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T23:15:05.976858Z",
     "start_time": "2020-09-03T23:15:05.967525Z"
    }
   },
   "outputs": [],
   "source": [
    "# Funcation that takes the year built and puts it into a bucket for decade\n",
    "\n",
    "def built_decades(x):\n",
    "#     Assigning the decade built\n",
    "\n",
    "    if x < 1910:\n",
    "        decade = 0\n",
    "    elif x < 1920 and x > 1909:\n",
    "        decade = 1\n",
    "    elif x < 1930 and x > 1919:\n",
    "        decade = 2\n",
    "    elif x < 1940 and x > 1929:\n",
    "        decade = 3\n",
    "    elif x < 1950 and x > 1939:\n",
    "        decade = 4\n",
    "    elif x < 1960 and x > 1949:\n",
    "        decade = 5\n",
    "    elif x < 1970 and x > 1959:\n",
    "        decade = 6\n",
    "    elif x < 1980 and x > 1969:\n",
    "        decade = 7\n",
    "    elif x < 1990 and x > 1979:\n",
    "        decade = 8\n",
    "    elif x < 2000 and x > 1989:\n",
    "        decade = 9\n",
    "    elif x < 2010 and x > 1999:\n",
    "        decade = 10\n",
    "    else:\n",
    "        decade = 11\n",
    "    return decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T23:15:06.685562Z",
     "start_time": "2020-09-03T23:15:06.669472Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mapping the function to the column to change the values\n",
    "df['yr_built'] = df['yr_built'].map(built_decades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T23:21:11.112125Z",
     "start_time": "2020-09-03T23:21:11.105074Z"
    }
   },
   "outputs": [],
   "source": [
    "df['yr_built'] = df['yr_built'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T23:21:13.824455Z",
     "start_time": "2020-09-03T23:21:13.813351Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking that the values changed\n",
    "df.yr_built.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T23:23:08.680108Z",
     "start_time": "2020-09-03T23:23:08.670121Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Question 2: What are the average home prices for the categorical features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have my categorical features picked, I wanted to look more closely at how they affect home prices. I decided to plot them all to see what kinds of interesting trends I would find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T23:30:30.801681Z",
     "start_time": "2020-09-03T23:30:30.796689Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function that plots each of the categorical features\n",
    "def plot_categories(df):\n",
    "    \n",
    "#     Loops through the category data types\n",
    "    for col in df.select_dtypes('category'):\n",
    "        \n",
    "#       Creating the figure and setting the fig size\n",
    "        fig, ax = plt.subplots(figsize=(10,6))\n",
    "    \n",
    "#       Plotting each column at a time\n",
    "        sns.barplot(x=col, y='price', data=df, palette=\"husl\")\n",
    "    \n",
    "#       Setting the title and formatting the x-ticks for better visibility\n",
    "        ax.set_title(f\"{col} vs Price\", fontsize=30)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T23:30:36.979967Z",
     "start_time": "2020-09-03T23:30:31.211228Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_categories(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From these graphs there are some interesting insights:\n",
    "* Condition shows that there is a division between a category of 1 or 2 and a category of 3, 4, or 5. One would expect a more linear relationship here, but it seems that it has created two buckets of lower and higher condition.\n",
    "* Year built is interesting that it shows a drop in price starting for houses in the 40s. I expected this to be a linear relationship but from this graph I would guess that many of the very oldest homes are desired for their uniqueness and perhaps certain craftsmanship that is iconic to the era. Then starting for houses built in the 40s, houses are not seen as iconic and charming anymore, and the newer the house the better. \n",
    "* The zipcode graph shows pretty much the same thing as the visual from before. There are huge differences in price based on zipcode.\n",
    "* I was expecting to see a greater difference among the graphs for month, week, and season sold. Winter has a slightly lower average price and February is the worst month. I was also expecting to see a bigger trend with the weeks that maybe it was better to sell a house and the beginning or very end of the month for example. It's interesting to see that the time of selling doesn't make as big of a difference as I thought it would."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made a dataframe specifically for modeling to leave my original clean dataframe untouched. I decided to drop latitude and longitude for modeling as zipcode will be a more meaningful location feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T23:49:54.479277Z",
     "start_time": "2020-09-03T23:49:54.469237Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating new modeling dataframe\n",
    "model1_df = df.copy()\n",
    "\n",
    "# Dropping columns\n",
    "model1_df.drop(columns=['lat', 'long'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T23:50:06.190117Z",
     "start_time": "2020-09-03T23:50:06.158203Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the new dataframe\n",
    "model1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T23:50:13.872438Z",
     "start_time": "2020-09-03T23:50:13.862367Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the new dataframe\n",
    "model1_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the modeling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T04:03:16.245661Z",
     "start_time": "2020-09-04T04:03:16.231393Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function that will run my model and some diagnostics\n",
    "\n",
    "def make_model(df, target='price', test_size=0.25, cv=20):\n",
    "\n",
    "#     Definging X and y\n",
    "    X = df.drop([target], axis=1)\n",
    "    y = df[[target]]\n",
    "    \n",
    "#     Performing a train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=123)\n",
    "    \n",
    "#     Creating dataframes with the split data\n",
    "    df_train = pd.concat([y_train, X_train], axis=1)\n",
    "    df_test = pd.concat([y_test, X_test], axis=1)\n",
    "    \n",
    "#     Pulling out the categorical columns\n",
    "    cat_cols = df_train.select_dtypes('category').columns\n",
    "    \n",
    "#     Writing the formula to feed into my model\n",
    "    features = '+'.join(df_train.drop(columns=target).columns)\n",
    "    \n",
    "#     Looping through the categoricals to format the formula correctly\n",
    "    for col in cat_cols:\n",
    "        features = features.replace(col,f'C({col})')\n",
    "        \n",
    "#     The completed formula to feed in to the model    \n",
    "    formula = target + '~' + features\n",
    "    \n",
    "#     Putting my training data through the model\n",
    "    model = smf.ols(formula, df_train).fit()\n",
    "    \n",
    "#     Plotting a qq plot of the residuals to check for normality\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(20, 5))\n",
    "    sm.graphics.qqplot(model.resid, fit=True, line='45', ax=axes[0])\n",
    "    axes[0].set_title('QQ Plot Normality Check', fontsize=20)\n",
    "    \n",
    "#     Plotting the residuals to check for homoscedasticity \n",
    "    ax=axes[1]\n",
    "    ax.scatter(df_train['price'], model.resid)\n",
    "    ax.axhline(0, color='red')\n",
    "    axes[1].set_title('Homoscedasticity Check', fontsize=20)\n",
    "    \n",
    "#   This will make both plots appear\n",
    "    plt.show();\n",
    "\n",
    "#     Getting the predicted y values from the model\n",
    "    y_predicted = model.predict(X_test)\n",
    "    \n",
    "#     Plotting a scatterplot of the predicted vs actual test data prices \n",
    "#     to visually inspect how different they are\n",
    "    plt.figure(figsize=(20,5))\n",
    "    \n",
    "#     Plotting the first 200 entries of the predicted and actual prices\n",
    "    g = sns.scatterplot(range(len(y_predicted[:200])), y_predicted[:200], \n",
    "                        label='Predicted Prices')\n",
    "    g = sns.scatterplot(range(len(y_test[:200])), y_test.price[:200], \n",
    "                        label='Actual Prices')\n",
    "\n",
    "#     Setting the title, labels, and legend of the plot\n",
    "    plt.title('Comparison of predicted vs actual price', \n",
    "              fontdict={'fontsize':20})\n",
    "    plt.xlabel('Values')\n",
    "    plt.ylabel('Prices')\n",
    "    plt.legend()\n",
    "    plt.show();\n",
    "    \n",
    "#   Getting the r2 for the test data to compare to the train data \n",
    "    r2_test = r2_score(y_test, y_predicted)\n",
    "    print('Model test data R2 score:', r2_test)\n",
    "    \n",
    "#   Doing a cross validation k-fold\n",
    "    cv_result = np.mean(cross_val_score(linreg, X, y, cv=cv, \n",
    "                                        scoring='neg_mean_squared_error'))\n",
    "    print('K-fold cross validation negative MSE:', cv_result)\n",
    "    \n",
    "#   Finally, displaying the model summary\n",
    "    display(model.summary())\n",
    "    \n",
    "#   The model is the object returned so I can perform different functions on it\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T04:03:20.309825Z",
     "start_time": "2020-09-04T04:03:18.026927Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1 = make_model(model1_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Insights for this model:\n",
    "* The r2 of the model is pretty high but the r2 of the test data is very off from the model which shows that it's not a very good model.\n",
    "* The qq plot and the homoscedasticity plot are both showing that the residuals do not have normality or homoscedasticity. \n",
    "* The scale on the plot of predicted vs actual prices is large and many of the values are hanging out at the bottom of the graph being reasonably close, but there are a few that are wildly off. \n",
    "* The cross validation score is negative MSE so the higher the number, the better. It is a very large negative number so I expect to see that number improve in future models.\n",
    "* Some p values are very significant, some are very not significant.\n",
    "* Overall this is just a starting point to compare to see how much I can improve various metrics while sparing model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model I'm only going to remove the outliers to see how that improves my model. Originally I experimented with removing outliers using z-scores, cook's distance, and interquartile range. The interquartile range improved my model the most so that is the method I stuck with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T01:14:05.724913Z",
     "start_time": "2020-09-04T01:14:05.720856Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to find the outliers according to the IQR method\n",
    "def iqr_outliers(data):\n",
    "    \n",
    "#     Defining the quartiles and finding the IQR\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "#     Defining the threshold and comparing the data against the threshold\n",
    "    threshold = iqr * 1.5\n",
    "    outliers = (data < q1 - threshold) | (data > q3 + threshold)\n",
    "    \n",
    "#     Making a series out of the outliers\n",
    "    outliers = pd.Series(outliers, index=data.index)\n",
    "        \n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T01:14:13.492330Z",
     "start_time": "2020-09-04T01:14:13.487330Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to add the outliers for each column as a new column in the dataframe\n",
    "def add_outliers_column(df, columns, verbose=True):\n",
    "    \n",
    "#     Makes a new dataframe to leave the previous unedited\n",
    "    new_df = df.copy()\n",
    "    \n",
    "#     Iterates through columns\n",
    "    for col in columns:\n",
    "        \n",
    "#         References the previous function to find the outliers\n",
    "        outliers = iqr_outliers(new_df[col])\n",
    "        \n",
    "#         Printing how many outliers were found in each column\n",
    "        if verbose:\n",
    "            print(f'{outliers.sum()} outliers found in {col}')\n",
    "    \n",
    "#         Adds the outliers as a new column\n",
    "        new_df[f'{col}_outliers'] = outliers\n",
    "        \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T01:14:56.614394Z",
     "start_time": "2020-09-04T01:14:56.561493Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feeding in the numerical columns for identifying outliers\n",
    "num_cols = list(model1_df.select_dtypes('number').columns)\n",
    "\n",
    "# Saving the new dataframe with outlier columns added\n",
    "(f'{outliers.sum()} outliers found')(f'{outliers.sum()} outliers found') = add_outliers_column(model1_df, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T01:15:01.780373Z",
     "start_time": "2020-09-04T01:15:01.725299Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking out my new dataframe\n",
    "model2_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I am going to drop the waterfront and was_renovated outlier columns since the outliers identified were all of the ones that are waterfront or were renovated. I don't want to lose those features completely. I'm also going to drop the outlier column for yr_renovated as the idea of outliers in that column doesn't make any sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T01:30:08.463255Z",
     "start_time": "2020-09-04T01:30:08.456253Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropping the selected new outlier columns\n",
    "model2_df.drop(columns=['waterfront_outliers', 'yr_renovated_outliers', \n",
    "                        'was_renovated_outliers'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T01:30:13.949650Z",
     "start_time": "2020-09-04T01:30:13.901631Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking my dataframe before outlier removal\n",
    "model2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T01:38:53.109606Z",
     "start_time": "2020-09-04T01:38:53.101483Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a list of the outlier columns\n",
    "outlier_cols = []\n",
    "for col in model2_df.columns:\n",
    "    if 'outliers' in col:\n",
    "        outlier_cols.append(col)\n",
    "outlier_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T01:42:47.031037Z",
     "start_time": "2020-09-04T01:42:46.984060Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filtering the outliers out of my dataframe and returning only the entries\n",
    "# that are not outliers for any of the features\n",
    "for col in outlier_cols:\n",
    "    model2_df = model2_df[(model2_df[col]==False)]\n",
    "model2_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T01:44:59.617455Z",
     "start_time": "2020-09-04T01:44:59.583456Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropping the outlier columns that were added\n",
    "for col in outlier_cols:\n",
    "    model2_df.drop(columns=[col], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T01:45:09.938251Z",
     "start_time": "2020-09-04T01:45:09.926322Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking my dataframe for the right data types and columns\n",
    "model2_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking linearity and normality after outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T01:53:07.975461Z",
     "start_time": "2020-09-04T01:53:07.704388Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting the figure and plotting\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "sns.distplot(model2_df['price'], bins='auto')\n",
    "\n",
    "# Adjusting the money ticks \n",
    "fmt_money = '${x:,.0f}'\n",
    "tick_money = mtick.StrMethodFormatter(fmt_money)\n",
    "ax.xaxis.set_major_formatter(tick_money)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Setting the title and labels\n",
    "ax.set_xlabel('Price', fontsize=15)\n",
    "ax.set_title('House Prices', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With the outliers removed from price there is a much more normal looking distribution than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T01:54:41.293122Z",
     "start_time": "2020-09-04T01:54:01.900026Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Looping through the columns to feed each into the function above\n",
    "cols = list(model2_df.columns)\n",
    "for col in cols:\n",
    "    explore_plot(model2_df, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Many of the features are more normally distributed with less dramatic tails. I decided to remove both sqft_lot and sqft_lot15 since there is not a good linear relationship there. I also decided to remove yr_renovated and sqft_basement as both of those columns are so full of zeros. Insteaqd I will rely on the was_renovated and has_basement columns to account for renovations and basements. I am hoping that removing these will improve my residual normality and homoscedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T03:32:37.783497Z",
     "start_time": "2020-09-04T03:32:37.779581Z"
    }
   },
   "outputs": [],
   "source": [
    "model2_df.drop(columns=['sqft_lot', 'sqft_lot15', 'yr_renovated', 'sqft_basement'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T03:32:42.086732Z",
     "start_time": "2020-09-04T03:32:42.054743Z"
    }
   },
   "outputs": [],
   "source": [
    "model2_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T03:32:55.334822Z",
     "start_time": "2020-09-04T03:32:53.179937Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model2 = make_model(model2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Insights for this model:\n",
    "* The r2 score has gone up by over 2% points and the r2 for the test data is much closer to the training data this time. \n",
    "* The residuals are greatly improved being more normally distributed and being more homoscedastic. There is still room for improvement on the residuals.\n",
    "* The scale on the comparison of the actual and predicted prices has gone way down so the two are much closer together. \n",
    "* The cross validation score has gone up by a lot.\n",
    "* Most p-values look significant but I have some wildly large p-values to address.\n",
    "* Overall, a great step of improvement. Let's see how I can improve it more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing low p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My next step is to address the features with low p-values. Any features that are categorical will be left in the modeling if the majority of the categories have a significant p-value. Likewise, the whole category will be removed if the majority have non-significant p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T03:33:26.054718Z",
     "start_time": "2020-09-04T03:33:26.046766Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to identify the insignificant p-values\n",
    "def bad_pvalues(model, verbose=True):\n",
    "    \n",
    "#     Pulling out the p-values and identifying the ones above .05\n",
    "    pvalues = model.pvalues\n",
    "    bad_features = pvalues[pvalues > .05]\n",
    "    \n",
    "#     Excluding the intercept in case it has a high p-value\n",
    "    if 'Intercept' in bad_features:\n",
    "        bad_features.remove('Intercept')\n",
    "        \n",
    "#      Printing a statement of the bad p-values\n",
    "    if verbose:\n",
    "        print(f'{len(bad_features)} bad p-values to be reviewed:\\n' + f'{bad_features}')\n",
    "    return bad_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T03:33:26.880748Z",
     "start_time": "2020-09-04T03:33:26.876748Z"
    }
   },
   "outputs": [],
   "source": [
    "model2_bad_pvals = bad_pvalues(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From this list I am going to remove week sold as none of the categories seem to be very significant. I am going to leave yr_built, month_sold, and zipcode alone as the majority of those categories are significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T03:34:07.776360Z",
     "start_time": "2020-09-04T03:34:07.767447Z"
    }
   },
   "outputs": [],
   "source": [
    "model3_df = model2_df.copy()\n",
    "model3_df.drop(columns=(['week_sold']), inplace=True)\n",
    "model3_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T03:34:13.489923Z",
     "start_time": "2020-09-04T03:34:11.821068Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model3 = make_model(model3_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Insights for this model:\n",
    "* I was hoping that my residuals would improve but they look the same as before.\n",
    "* The r2 is the same as before\n",
    "* Overall, removing the low p-value features didn't have very much impact on my model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My approach for this next model iteration is to address multicolinearity and VIF. I followed a guideline of about a 0.70 threshold for multicolinearity and 6 for VIF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicolinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T03:34:42.412748Z",
     "start_time": "2020-09-04T03:34:42.404811Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the dataframe for model iteration 4\n",
    "model4_df = model3_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T03:34:43.162765Z",
     "start_time": "2020-09-04T03:34:43.154036Z"
    }
   },
   "outputs": [],
   "source": [
    "# Writing a function to create a correlation heat map and chart\n",
    "def multicol_plot(df):\n",
    "    \n",
    "#   Creating a temporary new dataframe\n",
    "    new_df = df.copy()\n",
    "    \n",
    "#   Converting the category types to int so they will show up in the heatmap\n",
    "    categories = new_df.select_dtypes('category')\n",
    "    for col in categories:\n",
    "        new_df[col]= new_df[col].astype('int64')  \n",
    "        \n",
    "#   Generating the correlation chart \n",
    "    corr = abs(new_df.corr())\n",
    "    \n",
    "#   Creating a mask that will eliminate redundant values in the heatmap\n",
    "    mask = np.zeros_like(corr)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    \n",
    "#   Plotting the figure and applying the mask\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(12,12)\n",
    "    sns.heatmap(corr, annot=True, mask=mask)\n",
    "    plt.show();\n",
    "    \n",
    "    display(corr)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T03:34:44.808243Z",
     "start_time": "2020-09-04T03:34:44.055096Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Running the function\n",
    "model4_corr = multicol_plot(model4_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Interpretation of the results:\n",
    "* Squarefoot above and squarefoot living are highly correlated. I am going to keep squarefoot living since the overall squarefootage of a home is a more meaningful metric.\n",
    "* Squarefoot living and squarefoot living15 are also highly correlated but again I think that the squarefootage of the home is the more meaningful metric so I will eliminate the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T03:42:11.656384Z",
     "start_time": "2020-09-04T03:42:11.648426Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropping selected columns\n",
    "model4_df.drop(columns=['sqft_above', 'sqft_living15'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance Inflation Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T03:42:16.564779Z",
     "start_time": "2020-09-04T03:42:16.558750Z"
    }
   },
   "outputs": [],
   "source": [
    "# Writing a function to get vif scores for features\n",
    "def vif_results(df, target='price'):\n",
    "    \n",
    "#   Dropping the target and adding a constant\n",
    "    new_df = df.drop(columns=target, axis=1)\n",
    "    new_df = sm.add_constant(new_df)\n",
    "    \n",
    "#   Turning the category data types to integers so they show up in the VIF\n",
    "    categories = new_df.select_dtypes('category')\n",
    "    for col in categories:\n",
    "        new_df[col]= new_df[col].astype('int64')    \n",
    "\n",
    "#   Creating an empty list to append to  \n",
    "    vif_list = []\n",
    "#   Iterating through the columns of the dataframe\n",
    "    for x in range(new_df.shape[1]):\n",
    "        \n",
    "#       Running the vif for each column and adding it to the empty list\n",
    "        vif = variance_inflation_factor(new_df.values, x)\n",
    "        vif_list.append(vif)\n",
    "        \n",
    "#   Creating a series for the columns and the vif list\n",
    "    results = pd.Series(dict(zip(new_df.columns, vif_list)))\n",
    "    print(results)\n",
    "    \n",
    "#   Identifying features that are above the threshold\n",
    "    threshold = 6\n",
    "    bad_columns = list(results[results > threshold].index)\n",
    "    \n",
    "#   Removing the constant from being included in the bad features\n",
    "    if 'const' in bad_columns:\n",
    "        bad_columns.remove('const')\n",
    "        \n",
    "    return bad_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T03:42:17.617295Z",
     "start_time": "2020-09-04T03:42:17.399292Z"
    }
   },
   "outputs": [],
   "source": [
    "vif_columns = vif_results(model4_df)\n",
    "vif_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are no features above the threshold so I am not going to remove any more features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T03:42:24.796009Z",
     "start_time": "2020-09-04T03:42:23.202006Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model4 = make_model(model4_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Insights for this model:\n",
    "* The the graphs for the residuals look the same and have not improved.\n",
    "* The r2 score and cross validation score actually got a little bit worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log transforming price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My next tactic is going to be to log transform the price column to try to achieve better normality and homoscedasticity with my residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T03:46:23.626697Z",
     "start_time": "2020-09-04T03:46:23.623695Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a new dataframe for this model iteration\n",
    "model5_df = model4_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T03:46:24.099005Z",
     "start_time": "2020-09-04T03:46:24.069004Z"
    }
   },
   "outputs": [],
   "source": [
    "# Log transforming the price column\n",
    "model5_df['price'] = model5_df['price'].map(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking linearity and normality after log transforming price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T03:48:20.559664Z",
     "start_time": "2020-09-04T03:48:20.274691Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting the figure and plotting\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "sns.distplot(model5_df['price'], bins='auto')\n",
    "\n",
    "# Setting the title and labels\n",
    "ax.set_xlabel('Price', fontsize=15)\n",
    "ax.set_title('House Prices', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T03:51:32.269220Z",
     "start_time": "2020-09-04T03:51:08.375514Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking the distributions and linearity for my features again\n",
    "cols = list(model5_df.columns)\n",
    "for col in cols:\n",
    "    explore_plot(model5_df, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T04:03:30.684415Z",
     "start_time": "2020-09-04T04:03:29.042440Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model5 = make_model(model5_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Insights for this model:\n",
    "* The residuals for the qq plot have improved for the tail on the higher end, but the lower end have worsened. The bottom of the scale was -6 in the last model and now it is -8.\n",
    "* It's hard to judge if homoscedasticity has improved as the scale has also changed but there is still a little bit of a cone shape.\n",
    "* It is hard to tell if the comparison of the predicted and actual prices improved because the scale changed.\n",
    "* The r2 score went up by about .02 which is a good improvement.\n",
    "* The cross validation score greatly improved and is a much higher number than before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T04:08:57.568339Z",
     "start_time": "2020-09-04T04:08:57.560915Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for high p-values from this model\n",
    "model5_bad_pvals = bad_pvalues(model5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bedrooms now has an insignificant p value after log transforming price and will need to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T04:12:23.812785Z",
     "start_time": "2020-09-04T04:12:23.805858Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removing the bedrooms column from my next dataframe iteration\n",
    "model6_df = model5_df.copy()\n",
    "model6_df.drop(columns=['bedrooms'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicolinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T04:16:20.002078Z",
     "start_time": "2020-09-04T04:16:19.415037Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking if columns need to be addressed for multicolinearity again\n",
    "model6_corr = multicol_plot(model6_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nothing is above .70 so there is nothing to remove here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T04:17:06.530045Z",
     "start_time": "2020-09-04T04:17:06.352053Z"
    }
   },
   "outputs": [],
   "source": [
    "vif_columns = vif_results(model6_df)\n",
    "vif_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> No high VIF columns either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T04:19:45.782815Z",
     "start_time": "2020-09-04T04:19:44.080451Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model6 = make_model(model6_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last p-value check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T04:20:41.883093Z",
     "start_time": "2020-09-04T04:20:41.878994Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking p-values\n",
    "model6_bad_pvals = bad_pvalues(model6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are no features to removed based on p-values as these are all categories where the majority of the categories are significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **I am going to use this model iteration for my final model as I have done all I can to improve the model and the residuals while still keeping interpretability.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Question 3: Would the residuals improve from log transforming all the non categorical columns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I had such a hard time seeing improvement in the residuals for this project, I wanted to see what would the improvement look like if I log transformed all of the non categorical columns. Once all the columns are log transformed then I lose the ability to interpret the model so it's not practical for this project, but I wanted to see what the model would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T04:41:44.024229Z",
     "start_time": "2020-09-04T04:41:44.016229Z"
    }
   },
   "outputs": [],
   "source": [
    "# Making a copy of the dataframe for this iteration\n",
    "model7_df = model6_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T04:41:44.675675Z",
     "start_time": "2020-09-04T04:41:44.590691Z"
    }
   },
   "outputs": [],
   "source": [
    "# Grabbing the columns appropriate to transform\n",
    "num_cols = ['bathrooms', 'sqft_living', 'floors', 'grade']\n",
    "\n",
    "# Iterating through the number columns and log transforming them\n",
    "for col in num_cols:\n",
    "    model7_df[col] = model7_df[col].map(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T05:55:44.592762Z",
     "start_time": "2020-09-04T05:55:44.548762Z"
    }
   },
   "outputs": [],
   "source": [
    "model7_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T04:42:08.584291Z",
     "start_time": "2020-09-04T04:42:06.938289Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model7 = make_model(model7_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Insights from this model:\n",
    "* I am surprised that the model residuals did not improve. There was not a lot of change for better or worse.\n",
    "* I am also surprised that there was not a change in the r2 score or the cross validation score.\n",
    "* Overall the model quality is pretty much the same as the previous model but with a lot less interpretability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T04:46:56.174357Z",
     "start_time": "2020-09-04T04:46:56.166237Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for insignificant p-values\n",
    "model7_bad_pvals = bad_pvalues(model7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are more insignificant p-values in this model than in model 6 but still nothing that would justify removing. Again, I am surprised that there isn't a more dramatic difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
